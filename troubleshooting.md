# í¬ëª½ í¬ë¡¤ëŸ¬ í”„ë¡œì íŠ¸

í¬ëª½(Kmong) í”Œë«í¼ì—ì„œ íŒë§¤ì í”„ë¡œí•„, ë¦¬ë·°, ì„œë¹„ìŠ¤ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ì›¹ í¬ë¡¤ë§ ë„êµ¬ì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨
- [ì „ì²´ ì•„í‚¤í…ì²˜](#ì „ì²´-ì•„í‚¤í…ì²˜)
- [í´ë˜ìŠ¤ë³„ ì—­í• ](#í´ë˜ìŠ¤ë³„-ì—­í• )
- [ì„¤ì¹˜ ë° ì„¤ì •](#ì„¤ì¹˜-ë°-ì„¤ì •)
- [ë°ì´í„° ìˆ˜ì§‘ í”Œë¡œìš°](#ë°ì´í„°-ìˆ˜ì§‘-í”Œë¡œìš°)
- [ê¸°ëŠ¥ë³„ ì‚¬ìš©ë²•](#ê¸°ëŠ¥ë³„-ì‚¬ìš©ë²•)
- [ë¬¸ì œ í•´ê²° ê°€ì´ë“œ](#ë¬¸ì œ-í•´ê²°-ê°€ì´ë“œ)
- [ì•Œë ¤ì§„ ì´ìŠˆ](#ì•Œë ¤ì§„-ì´ìŠˆ)

## ğŸ—ï¸ ì „ì²´ ì•„í‚¤í…ì²˜

```
í¬ëª½ í¬ë¡¤ëŸ¬
â”œâ”€â”€ BaseCrawler (ê¸°ë³¸ í´ë˜ìŠ¤)
â”‚   â”œâ”€â”€ Selenium WebDriver ì„¤ì •
â”‚   â”œâ”€â”€ ë°ì´í„° ì €ì¥ ê¸°ëŠ¥
â”‚   â””â”€â”€ ë¸Œë¼ìš°ì € ê´€ë¦¬
â”‚
â”œâ”€â”€ SellersCrawler (íŒë§¤ì ìˆ˜ì§‘)
â”‚   â”œâ”€â”€ ì¹´í…Œê³ ë¦¬ë³„ íŒë§¤ì ëª©ë¡ í¬ë¡¤ë§
â”‚   â””â”€â”€ í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
â”‚
â””â”€â”€ ProfileCrawler (ìƒì„¸ ì •ë³´ ìˆ˜ì§‘)
    â”œâ”€â”€ íŒë§¤ì í”„ë¡œí•„ ì •ë³´
    â”œâ”€â”€ ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘
    â””â”€â”€ ì„œë¹„ìŠ¤ë³„ ìƒì„¸ ì •ë³´
```

### ë°ì´í„° íë¦„
1. **ì¹´í…Œê³ ë¦¬ íƒìƒ‰** â†’ íŒë§¤ì ëª©ë¡ ìˆ˜ì§‘
2. **íŒë§¤ì í”„ë¡œí•„** â†’ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
3. **ë¦¬ë·° ì„¹ì…˜** â†’ ê³ ê° ë¦¬ë·° ë°ì´í„°
4. **ì„œë¹„ìŠ¤ í˜ì´ì§€** â†’ ê°€ê²©, íŒ¨í‚¤ì§€ ì •ë³´

## ğŸ“¦ í´ë˜ìŠ¤ë³„ ì—­í• 

### BaseCrawler
**ì—­í• **: ëª¨ë“  í¬ë¡¤ëŸ¬ì˜ ê¸°ë³¸ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ë¶€ëª¨ í´ë˜ìŠ¤

**ì£¼ìš” ë©”ì„œë“œ**:
- `_setup_driver()`: Chrome WebDriver ì´ˆê¸°í™” ë° ì˜µì…˜ ì„¤ì •
- `save_data(data, filename)`: ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
- `close()`: ë¸Œë¼ìš°ì € ì„¸ì…˜ ì¢…ë£Œ

**í•µì‹¬ ì„¤ì •**:
```python
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
options.add_argument('--disable-blink-features=AutomationControlled')
options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')
```

### SellersCrawler
**ì—­í• **: ì¹´í…Œê³ ë¦¬ë³„ íŒë§¤ì ëª©ë¡ì„ ìˆ˜ì§‘í•˜ëŠ” í´ë˜ìŠ¤

**ì£¼ìš” ë©”ì„œë“œ**:
- `crawl_category_sellers(category_id)`: íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë“  íŒë§¤ì ìˆ˜ì§‘
- `crawl_multiple_categories(category_ids)`: ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ ì¼ê´„ ì²˜ë¦¬
- `_extract_sellers_from_page()`: í˜ì´ì§€ë³„ íŒë§¤ì ì •ë³´ ì¶”ì¶œ

**ìˆ˜ì§‘ ë°ì´í„°**:
- íŒë§¤ì ì´ë¦„
- ì¹´í…Œê³ ë¦¬ ID
- ì„œë¹„ìŠ¤ ì¸ë±ìŠ¤

### ProfileCrawler
**ì—­í• **: íŒë§¤ìì˜ ìƒì„¸ í”„ë¡œí•„, ë¦¬ë·°, ì„œë¹„ìŠ¤ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í´ë˜ìŠ¤

**ì£¼ìš” ë©”ì„œë“œ**:

#### í”„ë¡œí•„ ì •ë³´ ìˆ˜ì§‘
- `crawl_seller_profile(seller_name)`: ê¸°ë³¸ í”„ë¡œí•„ ì •ë³´ ìˆ˜ì§‘
- `_extract_introduction()`: ìê¸°ì†Œê°œ ì¶”ì¶œ
- `_extract_career()`: ê²½ë ¥ì‚¬í•­ ì¶”ì¶œ
- `_extract_skills()`: ë³´ìœ  ê¸°ìˆ  ì¶”ì¶œ
- `_extract_specialties()`: ITÂ·í”„ë¡œê·¸ë˜ë° ì „ë¬¸ë¶„ì•¼ ì¶”ì¶œ

#### ë¦¬ë·° ìˆ˜ì§‘
- `crawl_reviews(seller_name, max_pages)`: ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘
- `_extract_reviews_from_page()`: í˜ì´ì§€ë³„ ë¦¬ë·° ì¶”ì¶œ
- `_go_to_next_review_page()`: ë¦¬ë·° í˜ì´ì§€ë„¤ì´ì…˜

#### ì„œë¹„ìŠ¤ ì •ë³´ ìˆ˜ì§‘ (í˜„ì¬ ì´ìŠˆ ìˆìŒ)
- `crawl_services(seller_name)`: íŒë§¤ìì˜ ì„œë¹„ìŠ¤ ëª©ë¡ ìˆ˜ì§‘
- `crawl_single_service(service_url)`: ê°œë³„ ì„œë¹„ìŠ¤ ìƒì„¸ ì •ë³´
- `_extract_package_prices()`: íŒ¨í‚¤ì§€ë³„ ê°€ê²© ì •ë³´
- `_extract_skill_level()`: ê¸°ìˆ  ìˆ˜ì¤€ ì •ë³´
- `_extract_team_size()`: íŒ€ ê·œëª¨ ì •ë³´

#### í†µí•© ê¸°ëŠ¥
- `crawl_seller_profile_complete()`: í”„ë¡œí•„ + ë¦¬ë·° + ì„œë¹„ìŠ¤ ì „ì²´ ìˆ˜ì§‘

## ğŸš€ ì„¤ì¹˜ ë° ì„¤ì •

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­
```bash
pip install selenium pandas
```

### Chrome WebDriver ì„¤ì •
1. Chrome ë¸Œë¼ìš°ì € ì„¤ì¹˜
2. ChromeDriver ë‹¤ìš´ë¡œë“œ ë° PATH ì„¤ì •
3. ë˜ëŠ” `webdriver-manager` ì‚¬ìš©:
```bash
pip install webdriver-manager
```

### ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
project/
â”œâ”€â”€ crawler.py
â”œâ”€â”€ output/           # CSV ì¶œë ¥ íŒŒì¼ë“¤
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ í”Œë¡œìš°

### 1. íŒë§¤ì ëª©ë¡ ìˆ˜ì§‘
```
ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì ‘ê·¼ â†’ ì„œë¹„ìŠ¤ ì¹´ë“œ íƒì§€ â†’ íŒë§¤ìëª… ì¶”ì¶œ â†’ í˜ì´ì§€ë„¤ì´ì…˜ â†’ CSV ì €ì¥
```

### 2. í”„ë¡œí•„ ì •ë³´ ìˆ˜ì§‘
```
í”„ë¡œí•„ URL ìƒì„± â†’ ì„¹ì…˜ë³„ ì •ë³´ ì¶”ì¶œ â†’ êµ¬ì¡°í™”ëœ ë°ì´í„° ë³€í™˜ â†’ ì €ì¥
```

### 3. ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘
```
í”„ë¡œí•„ í˜ì´ì§€ â†’ ë¦¬ë·° ì„¹ì…˜ ìŠ¤í¬ë¡¤ â†’ í˜ì´ì§€ë³„ ë¦¬ë·° ì¶”ì¶œ â†’ ë‹¤ìŒ í˜ì´ì§€ ì´ë™ â†’ ì €ì¥
```

### 4. ì„œë¹„ìŠ¤ ì •ë³´ ìˆ˜ì§‘ (í˜„ì¬ ì´ìŠˆ)
```
ì„œë¹„ìŠ¤ íƒ­ í´ë¦­ â†’ ì„œë¹„ìŠ¤ URL ëª©ë¡ â†’ ê°œë³„ ì„œë¹„ìŠ¤ í˜ì´ì§€ â†’ íŒ¨í‚¤ì§€/ê°€ê²© ì¶”ì¶œ â†’ ì €ì¥
```

## ğŸ’» ê¸°ëŠ¥ë³„ ì‚¬ìš©ë²•

### ê¸°ë³¸ ì‚¬ìš©ë²•

#### 1. ì¹´í…Œê³ ë¦¬ë³„ íŒë§¤ì ìˆ˜ì§‘
```python
from crawler import SellersCrawler

# í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
sellers_crawler = SellersCrawler()

# ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
category_id = 661  # ITÂ·í”„ë¡œê·¸ë˜ë°
sellers = sellers_crawler.crawl_category_sellers(category_id)

# ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
category_ids = [661, 662, 663]
all_sellers = sellers_crawler.crawl_multiple_categories(category_ids)

# ì¢…ë£Œ
sellers_crawler.close()
```

#### 2. í”„ë¡œí•„ ì •ë³´ ìˆ˜ì§‘
```python
from crawler import ProfileCrawler

# í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
profile_crawler = ProfileCrawler()

# ë‹¨ì¼ í”„ë¡œí•„ í¬ë¡¤ë§
profile_data = profile_crawler.crawl_seller_profile("seller_username")

# CSVì—ì„œ íŒë§¤ì ëª©ë¡ ì½ì–´ì„œ í¬ë¡¤ë§
profiles = profile_crawler.crawl_from_csv('output/category_661_sellers.csv', limit=10)

# ì¢…ë£Œ
profile_crawler.close()
```

#### 3. ë¦¬ë·° í¬í•¨ í¬ë¡¤ë§
```python
# í”„ë¡œí•„ + ë¦¬ë·° í†µí•© ìˆ˜ì§‘
profile_with_reviews = profile_crawler.crawl_seller_profile_with_reviews(
    seller_name="seller_username", 
    max_review_pages=5
)

# ì—¬ëŸ¬ íŒë§¤ì ë¦¬ë·° í¬í•¨ í¬ë¡¤ë§
seller_names = ["seller1", "seller2", "seller3"]
all_data = profile_crawler.crawl_multiple_profiles_with_reviews(
    seller_names, 
    max_review_pages=3
)
```

#### 4. ì „ì²´ ë°ì´í„° ìˆ˜ì§‘
```python
# í”„ë¡œí•„ + ë¦¬ë·° + ì„œë¹„ìŠ¤ í†µí•© ìˆ˜ì§‘
complete_data = profile_crawler.crawl_seller_profile_complete(
    seller_name="seller_username",
    max_review_pages=2
)
```

### ê³ ê¸‰ ì‚¬ìš©ë²•

#### ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ
```python
def crawl_category_batch(category_id, max_profiles=50):
    """ì¹´í…Œê³ ë¦¬ ì „ì²´ ë°°ì¹˜ ì²˜ë¦¬"""
    
    # 1ë‹¨ê³„: íŒë§¤ì ëª©ë¡ ìˆ˜ì§‘
    sellers_crawler = SellersCrawler()
    sellers = sellers_crawler.crawl_category_sellers(category_id)
    sellers_crawler.close()
    
    # 2ë‹¨ê³„: í”„ë¡œí•„ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    profile_crawler = ProfileCrawler()
    seller_names = [s['seller_name'] for s in sellers[:max_profiles]]
    
    profiles = profile_crawler.crawl_multiple_profiles_with_reviews(
        seller_names, 
        max_review_pages=2
    )
    profile_crawler.close()
    
    return profiles
```

## ğŸ”§ ìˆ˜ì§‘ë˜ëŠ” ë°ì´í„° êµ¬ì¡°

### íŒë§¤ì ê¸°ë³¸ ì •ë³´
```json
{
    "seller_name": "íŒë§¤ìëª…",
    "profile_url": "https://kmong.com/@seller_name",
    "introduction": "ìê¸°ì†Œê°œ í…ìŠ¤íŠ¸",
    "career": ["ê²½ë ¥1", "ê²½ë ¥2"],
    "specialties": ["ITÂ·í”„ë¡œê·¸ë˜ë°", "ì„¸ë¶€ë¶„ì•¼1", "ì„¸ë¶€ë¶„ì•¼2"],
    "skills": ["ê¸°ìˆ 1", "ê¸°ìˆ 2"]
}
```

### ë¦¬ë·° ë°ì´í„°
```json
{
    "reviews": [
        {
            "review_date": "ì‘ì„±ì¼ì",
            "service_title": "ì„œë¹„ìŠ¤ëª…",
            "work_period": "ì‘ì—…ê¸°ê°„",
            "order_amount": "ì£¼ë¬¸ê¸ˆì•¡"
        }
    ],
    "total_reviews": 10
}
```

### ì„œë¹„ìŠ¤ ì •ë³´ (í˜„ì¬ ì´ìŠˆ)
```json
{
    "services": [
        {
            "service_url": "ì„œë¹„ìŠ¤ URL",
            "packages": {
                "STANDARD": "ê¸°ë³¸ ê°€ê²©",
                "DELUXE": "ë””ëŸ­ìŠ¤ ê°€ê²©", 
                "PREMIUM": "í”„ë¦¬ë¯¸ì—„ ê°€ê²©"
            },
            "skill_level": "ìˆ™ë ¨ë„",
            "team_size": "íŒ€ ê·œëª¨"
        }
    ],
    "total_services": 5
}
```

## ğŸ› ï¸ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. WebDriver ê´€ë ¨ ì˜¤ë¥˜
**ì¦ìƒ**: `selenium.common.exceptions.WebDriverException`
**í•´ê²°ì±…**:
```bash
# ChromeDriver ì—…ë°ì´íŠ¸
pip install --upgrade selenium
# ë˜ëŠ” webdriver-manager ì‚¬ìš©
pip install webdriver-manager
```

#### 2. ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜
**ì¦ìƒ**: `NoSuchElementException`
**í•´ê²°ì±…**:
- `time.sleep()` ì‹œê°„ ì¦ê°€
- `WebDriverWait` ì‚¬ìš©
- XPath ì„ íƒì ì¬ê²€í† 

#### 3. í˜ì´ì§€ë„¤ì´ì…˜ ì‹¤íŒ¨
**ì¦ìƒ**: ë¬´í•œ ë£¨í”„ ë˜ëŠ” í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨
**í•´ê²°ì±…**:
- `max_attempts` ì œí•œ ì„¤ì • í™•ì¸
- JavaScript í´ë¦­ ì‚¬ìš©: `driver.execute_script("arguments[0].click();", element)`

### ì„œë¹„ìŠ¤ ìˆ˜ì§‘ ê¸°ëŠ¥ ì´ìŠˆ í•´ê²°

#### í˜„ì¬ ë¬¸ì œì 
ì„œë¹„ìŠ¤ë³„ ì •ë³´ ìˆ˜ì§‘ ê¸°ëŠ¥ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œë“¤ì´ ë°œìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤:

1. **íŒ¨í‚¤ì§€ ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨**
   - ë™ì  ë¡œë”©ìœ¼ë¡œ ì¸í•œ ìš”ì†Œ ì ‘ê·¼ ì–´ë ¤ì›€
   - íŒ¨í‚¤ì§€ ë²„íŠ¼ í´ë¦­ í›„ ê°€ê²© ì •ë³´ ë¡œë”© ì§€ì—°

2. **ì„œë¹„ìŠ¤ ëª©ë¡ URL ìˆ˜ì§‘ ë¶ˆì•ˆì •**
   - ì„œë¹„ìŠ¤ íƒ­ í™œì„±í™” ë¬¸ì œ
   - ì¤‘ë³µ URL í•„í„°ë§ ì´ìŠˆ

#### ê¶Œì¥ í•´ê²° ë°©ì•ˆ

##### 1. ëŒ€ê¸° ì‹œê°„ ì¦ê°€
```python
def _extract_package_prices(self):
    # ê¸°ì¡´: time.sleep(2)
    time.sleep(5)  # ì¦ê°€
    
    # WebDriverWait í™œìš©
    WebDriverWait(self.driver, 15).until(
        EC.presence_of_element_located((By.CLASS_NAME, "price-class"))
    )
```

##### 2. ë” ì•ˆì •ì ì¸ ì„ íƒì ì‚¬ìš©
```python
def _find_price_with_multiple_selectors(self):
    # í˜„ì¬ ì—¬ëŸ¬ ì„ íƒìë¥¼ ì‹œë„í•˜ì§€ë§Œ, ë” ì¶”ê°€ í•„ìš”
    price_selectors = [
        # ê¸°ì¡´ ì„ íƒìë“¤...
        "//span[contains(@class, 'price') and contains(text(), 'ì›')]",
        "//*[@data-testid='price-display']",
        "//div[contains(@class, 'package-price')]//span"
    ]
    
    for selector in price_selectors:
        try:
            element = self.driver.find_element(By.XPATH, selector)
            if element.text.strip():
                return element.text.strip()
        except:
            continue
    return ""
```

##### 3. ì„œë¹„ìŠ¤ íƒ­ í™œì„±í™” ê°œì„ 
```python
def crawl_services(self, seller_name):
    try:
        self.driver.get(f"https://kmong.com/@{seller_name}")
        time.sleep(3)
        
        # ë” ì•ˆì •ì ì¸ íƒ­ ì„ íƒ
        service_tabs = [
            "//button[contains(text(), 'ì„œë¹„ìŠ¤')]",
            "//button[contains(text(), 'í¬íŠ¸í´ë¦¬ì˜¤')]", 
            "//a[contains(@href, 'portfolio')]",
            "//*[@role='tab' and contains(text(), 'ì„œë¹„ìŠ¤')]"
        ]
        
        tab_clicked = False
        for tab_selector in service_tabs:
            try:
                tab = self.driver.find_element(By.XPATH, tab_selector)
                self.driver.execute_script("arguments[0].click();", tab)
                time.sleep(3)
                tab_clicked = True
                break
            except:
                continue
        
        if not tab_clicked:
            print("ì„œë¹„ìŠ¤ íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
    except Exception as e:
        print(f"ì„œë¹„ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
```

##### 4. ë™ì  ì½˜í…ì¸  ëŒ€ì‘
```python
def _wait_for_dynamic_content(self, selector, timeout=10):
    """ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°"""
    try:
        WebDriverWait(self.driver, timeout).until(
            lambda driver: len(driver.find_elements(By.XPATH, selector)) > 0
        )
        return True
    except:
        return False
```

## ğŸ¯ ê¸°ëŠ¥ë³„ ì‚¬ìš©ë²•

### 1. ê¸°ë³¸ í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš°

```python
# ì „ì²´ ì›Œí¬í”Œë¡œìš° ì˜ˆì œ
def main_workflow():
    # Step 1: ì¹´í…Œê³ ë¦¬ë³„ íŒë§¤ì ìˆ˜ì§‘
    sellers_crawler = SellersCrawler()
    sellers = sellers_crawler.crawl_category_sellers(661)  # ITÂ·í”„ë¡œê·¸ë˜ë°
    sellers_crawler.close()
    
    # Step 2: ìƒìœ„ Nëª… í”„ë¡œí•„ ìƒì„¸ ìˆ˜ì§‘
    profile_crawler = ProfileCrawler()
    top_sellers = [s['seller_name'] for s in sellers[:20]]
    
    profiles = profile_crawler.crawl_multiple_profiles_with_reviews(
        top_sellers, 
        max_review_pages=3
    )
    
    # Step 3: ì„œë¹„ìŠ¤ ì •ë³´ ìˆ˜ì§‘ (ì„ íƒì )
    for seller_name in top_sellers[:5]:  # ì†Œìˆ˜ë§Œ í…ŒìŠ¤íŠ¸
        try:
            services = profile_crawler.crawl_services(seller_name)
            print(f"{seller_name}: {len(services)}ê°œ ì„œë¹„ìŠ¤")
        except Exception as e:
            print(f"{seller_name} ì„œë¹„ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    profile_crawler.close()

if __name__ == "__main__":
    main_workflow()
```

### 2. CSV ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬

```python
# CSVì—ì„œ íŒë§¤ì ëª©ë¡ì„ ì½ì–´ì™€ì„œ í”„ë¡œí•„ ìˆ˜ì§‘
def batch_from_csv(csv_path, limit=None):
    profile_crawler = ProfileCrawler()
    
    try:
        profiles = profile_crawler.crawl_from_csv(csv_path, limit=limit)
        print(f"ì´ {len(profiles)}ëª…ì˜ í”„ë¡œí•„ ìˆ˜ì§‘ ì™„ë£Œ")
        return profiles
    finally:
        profile_crawler.close()

# ì‚¬ìš© ì˜ˆ
profiles = batch_from_csv('output/category_661_sellers.csv', limit=10)
```

### 3. ì—ëŸ¬ ë³µêµ¬ ë° ì¬ì‹œë„

```python
def robust_crawl_with_retry(seller_names, max_retries=3):
    """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ì•ˆì •ì ì¸ í¬ë¡¤ë§"""
    profile_crawler = ProfileCrawler()
    successful_profiles = []
    failed_sellers = []
    
    for seller_name in seller_names:
        retries = 0
        success = False
        
        while retries < max_retries and not success:
            try:
                profile = profile_crawler.crawl_seller_profile(seller_name)
                if profile and 'error' not in profile:
                    successful_profiles.append(profile)
                    success = True
                else:
                    retries += 1
                    time.sleep(5)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                    
            except Exception as e:
                print(f"{seller_name} ì‹œë„ {retries + 1} ì‹¤íŒ¨: {e}")
                retries += 1
                time.sleep(5)
        
        if not success:
            failed_sellers.append(seller_name)
    
    profile_crawler.close()
    return successful_profiles, failed_sellers
```

## âš ï¸ ì•Œë ¤ì§„ ì´ìŠˆ

### ğŸ”´ ì„œë¹„ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ê¸°ëŠ¥ ë¬¸ì œ

**í˜„ì¬ ìƒíƒœ**: ë¶€ë¶„ì ìœ¼ë¡œ ì‘ë™í•˜ì§€ ì•ŠìŒ

**ì£¼ìš” ì´ìŠˆë“¤**:

1. **íŒ¨í‚¤ì§€ ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨**
   - ë™ì  ë¡œë”©ìœ¼ë¡œ ì¸í•œ ì§€ì—°
   - íŒ¨í‚¤ì§€ ë²„íŠ¼ í´ë¦­ í›„ DOM ì—…ë°ì´íŠ¸ ëŒ€ê¸° ì‹œê°„ ë¶€ì¡±
   - ê°€ê²© í‘œì‹œ ìš”ì†Œì˜ XPath ë³€ê²½

2. **ì„œë¹„ìŠ¤ íƒ­ í™œì„±í™” ë¬¸ì œ**
   - ì¼ë¶€ í”„ë¡œí•„ì—ì„œ ì„œë¹„ìŠ¤ íƒ­ í´ë¦­ ì‹¤íŒ¨
   - íƒ­ ì „í™˜ í›„ ì½˜í…ì¸  ë¡œë”© ì§€ì—°

3. **ì„œë¹„ìŠ¤ URL ìˆ˜ì§‘ ë¶ˆì•ˆì •**
   - ì¤‘ë³µ URL í•„í„°ë§ ë¡œì§ ê°œì„  í•„ìš”
   - href ì†ì„± ì¶”ì¶œ ì‹œ ìƒëŒ€/ì ˆëŒ€ ê²½ë¡œ ì²˜ë¦¬

**ì„ì‹œ í•´ê²°ì±…**:
- ì„œë¹„ìŠ¤ ì •ë³´ ìˆ˜ì§‘ì€ ì†ŒëŸ‰(5ê°œ ì´í•˜)ìœ¼ë¡œ ì œí•œí•˜ì—¬ í…ŒìŠ¤íŠ¸
- ëŒ€ê¸° ì‹œê°„ì„ 2ë°°ë¡œ ì¦ê°€ (`time.sleep(5)`)
- ìˆ˜ë™ìœ¼ë¡œ ì„œë¹„ìŠ¤ URLì„ ë¯¸ë¦¬ ìˆ˜ì§‘ í›„ ê°œë³„ ì²˜ë¦¬

### ğŸŸ¡ ê¸°íƒ€ ì•Œë ¤ì§„ ì œí•œì‚¬í•­

1. **ì†ë„ ì œí•œ**
   - í¬ëª½ ì„œë²„ì˜ ìš”ì²­ ì œí•œìœ¼ë¡œ ì¸í•œ ì§€ì—° í•„ìš”
   - ê¶Œì¥: ìš”ì²­ ê°„ 2-3ì´ˆ ëŒ€ê¸°

2. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**
   - ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì‹œ ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥
   - ê¶Œì¥: ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¤‘ê°„ ì €ì¥

3. **ë¸Œë¼ìš°ì € ì•ˆì •ì„±**
   - ì¥ì‹œê°„ ì‹¤í–‰ ì‹œ ë¸Œë¼ìš°ì € í¬ë˜ì‹œ ê°€ëŠ¥
   - ê¶Œì¥: 50-100ê°œ í”„ë¡œí•„ë§ˆë‹¤ ë¸Œë¼ìš°ì € ì¬ì‹œì‘

## ğŸ”§ ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§

### ë¡œê·¸ ì¶œë ¥ í•´ì„
```python
# ì •ìƒì ì¸ ë¡œê·¸ íŒ¨í„´
"ì¹´í…Œê³ ë¦¬ 661, í˜ì´ì§€ 1 í¬ë¡¤ë§ ì¤‘..."
"í˜„ì¬ í˜ì´ì§€ì—ì„œ 20ê°œ ì„œë¹„ìŠ¤ ë°œê²¬"
"í˜ì´ì§€ 1ì—ì„œ 15ëª… ìˆ˜ì§‘"
"í”„ë¡œí•„ í¬ë¡¤ë§ ì™„ë£Œ: seller_name"

# ë¬¸ì œ ìƒí™© ë¡œê·¸
"ì„œë¹„ìŠ¤ ê°œìˆ˜: 0ê°œì˜ ì„œë¹„ìŠ¤"  # ì¹´í…Œê³ ë¦¬ ë
"ì„œë¹„ìŠ¤ ì¹´ë“œ ì°¾ê¸° ì‹¤íŒ¨"      # DOM êµ¬ì¡° ë³€ê²½
"ê°€ê²©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"       # ì„œë¹„ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨
```

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```python
import time

def timed_crawl(func, *args, **kwargs):
    """ì‹¤í–‰ ì‹œê°„ ì¸¡ì •"""
    start_time = time.time()
    result = func(*args, **kwargs)
    end_time = time.time()
    
    print(f"ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
    return result

# ì‚¬ìš© ì˜ˆ
profiles = timed_crawl(
    profile_crawler.crawl_multiple_profiles, 
    seller_names
)
```

## ğŸ“ˆ ìµœì í™” ê¶Œì¥ì‚¬í•­

### 1. íš¨ìœ¨ì ì¸ ë°ì´í„° ìˆ˜ì§‘ ì „ëµ
```python
# ë‹¨ê³„ì  ìˆ˜ì§‘ ì ‘ê·¼ë²•
def staged_collection():
    # 1ë‹¨ê³„: íŒë§¤ì ëª©ë¡ë§Œ ë¹ ë¥´ê²Œ ìˆ˜ì§‘
    collect_seller_lists()
    
    # 2ë‹¨ê³„: ìƒìœ„ íŒë§¤ì í”„ë¡œí•„ ìš°ì„  ìˆ˜ì§‘
    collect_top_profiles()
    
    # 3ë‹¨ê³„: ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘
    collect_reviews_data()
    
    # 4ë‹¨ê³„: ì„œë¹„ìŠ¤ ì •ë³´ (ë¬¸ì œ í•´ê²° í›„)
    # collect_service_details()
```

### 2. ë©”ëª¨ë¦¬ ê´€ë¦¬
```python
# ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ
def memory_efficient_crawl(seller_names, batch_size=10):
    profile_crawler = ProfileCrawler()
    
    for i in range(0, len(seller_names), batch_size):
        batch = seller_names[i:i+batch_size]
        profiles = profile_crawler.crawl_multiple_profiles(batch)
        
        # ì¦‰ì‹œ ì €ì¥í•˜ê³  ë©”ëª¨ë¦¬ í•´ì œ
        profile_crawler.save_data(profiles, f'batch_{i//batch_size + 1}')
        del profiles  # ëª…ì‹œì  ë©”ëª¨ë¦¬ í•´ì œ
    
    profile_crawler.close()
```

### 3. ì˜¤ë¥˜ ë³µêµ¬ ì „ëµ
```python
def resilient_crawl(seller_names):
    profile_crawler = ProfileCrawler()
    results = []
    errors = []
    
    for seller_name in seller_names:
        try:
            profile = profile_crawler.crawl_seller_profile(seller_name)
            results.append(profile)
            
        except Exception as e:
            error_info = {
                'seller_name': seller_name,
                'error': str(e),
                'timestamp': time.time()
            }
            errors.append(error_info)
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¸Œë¼ìš°ì € ì¬ì‹œì‘
            try:
                profile_crawler.close()
                profile_crawler = ProfileCrawler()
            except:
                pass
    
    # ì˜¤ë¥˜ ë¡œê·¸ ì €ì¥
    if errors:
        profile_crawler.save_data(errors, 'crawl_errors')
    
    profile_crawler.close()
    return results, errors
```

## ğŸ“ ê°œë°œ ìš°ì„ ìˆœìœ„

### ì¦‰ì‹œ í•´ê²° í•„ìš” (High Priority)
1. **ì„œë¹„ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ê¸°ëŠ¥ ìˆ˜ì •**
   - íŒ¨í‚¤ì§€ ê°€ê²© ì¶”ì¶œ ë¡œì§ ê°œì„ 
   - ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¡°ì •
   - ì„œë¹„ìŠ¤ íƒ­ í™œì„±í™” ì•ˆì •ì„± í–¥ìƒ

### ì¤‘ê¸° ê°œì„  ì‚¬í•­ (Medium Priority)
1. **ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”**
   - ìë™ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
   - ìƒì„¸í•œ ì˜¤ë¥˜ ë¡œê¹…
   - ë¶€ë¶„ ì‹¤íŒ¨ ì‹œ ë³µêµ¬ ë¡œì§

2. **ì„±ëŠ¥ ìµœì í™”**
   - ë³‘ë ¬ ì²˜ë¦¬ ë„ì…
   - ë¶ˆí•„ìš”í•œ ëŒ€ê¸° ì‹œê°„ ìµœì í™”
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°œì„ 

### ì¥ê¸° ëª©í‘œ (Low Priority)
1. **ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**
2. **ì„¤ì • íŒŒì¼ ê¸°ë°˜ ê´€ë¦¬**
3. **API ì¸í„°í˜ì´ìŠ¤ ì¶”ê°€**

## ğŸ” ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­

1. **ìš”ì²­ ì œí•œ ì¤€ìˆ˜**: í¬ëª½ ì„œë²„ì— ê³¼ë„í•œ ë¶€í•˜ë¥¼ ì£¼ì§€ ì•Šë„ë¡ ì ì ˆí•œ ì§€ì—° ì‹œê°„ ìœ ì§€
2. **ë¡œë´‡ ê°ì§€ íšŒí”¼**: User-Agent ì„¤ì • ë° ìì—°ìŠ¤ëŸ¬ìš´ ë¸Œë¼ìš°ì§• íŒ¨í„´ ìœ ì§€
3. **ë°ì´í„° ë°±ì—…**: ì¥ì‹œê°„ í¬ë¡¤ë§ ì‹œ ì¤‘ê°„ ì €ì¥ í•„ìˆ˜
4. **ë²•ì  ì¤€ìˆ˜**: í¬ëª½ ì´ìš©ì•½ê´€ ë° robots.txt ì¤€ìˆ˜

## ğŸ“Š ì¶œë ¥ íŒŒì¼ êµ¬ì¡°

### ìƒì„±ë˜ëŠ” CSV íŒŒì¼ë“¤

```
output/
â”œâ”€â”€ category_{id}_sellers.csv          # ì¹´í…Œê³ ë¦¬ë³„ íŒë§¤ì ëª©ë¡
â”œâ”€â”€ profiles_batch_{n}.csv             # í”„ë¡œí•„ ë°°ì¹˜ë³„ ì €ì¥
â”œâ”€â”€ profiles_with_reviews_batch_{n}.csv # ë¦¬ë·° í¬í•¨ í”„ë¡œí•„
â”œâ”€â”€ all_profiles.csv                   # ì „ì²´ í”„ë¡œí•„ í†µí•©
â””â”€â”€ crawl_errors.csv                   # ì˜¤ë¥˜ ë¡œê·¸
```

### CSV íŒŒì¼ í•„ë“œ ì„¤ëª…

#### category_{id}_sellers.csv
| í•„ë“œ | ì„¤ëª… | ì˜ˆì‹œ |
|------|------|------|
| seller_name | íŒë§¤ì ì‚¬ìš©ìëª… | "developer123" |
| service_index | í˜ì´ì§€ ë‚´ ì„œë¹„ìŠ¤ ìˆœì„œ | 1, 2, 3... |
| category_id | ì¹´í…Œê³ ë¦¬ ID | 661 |

#### all_profiles.csv  
| í•„ë“œ | ì„¤ëª… | íƒ€ì… |
|------|------|------|
| seller_name | íŒë§¤ìëª… | String |
| profile_url | í”„ë¡œí•„ URL | String |
| introduction | ìê¸°ì†Œê°œ | String |
| career | ê²½ë ¥ì‚¬í•­ | List |
| specialties | IT ì „ë¬¸ë¶„ì•¼ | List |
| skills | ë³´ìœ  ê¸°ìˆ  | List |
| reviews | ë¦¬ë·° ë°ì´í„° | List |
| total_reviews | ì´ ë¦¬ë·° ìˆ˜ | Integer |
| services | ì„œë¹„ìŠ¤ ì •ë³´ | List |
| total_services | ì´ ì„œë¹„ìŠ¤ ìˆ˜ | Integer |

## âš™ï¸ ì„¤ì • ë° ì»¤ìŠ¤í„°ë§ˆì´ì§•

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```python
# crawler_config.py
class CrawlerConfig:
    # ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    DEFAULT_WAIT = 3
    LONG_WAIT = 5
    
    # í˜ì´ì§€ ì œí•œ
    MAX_PAGES_PER_CATEGORY = 100
    MAX_REVIEW_PAGES = 5
    
    # ë°°ì¹˜ í¬ê¸°
    BATCH_SIZE = 10
    
    # ì¬ì‹œë„ ì„¤ì •
    MAX_RETRIES = 3
    RETRY_DELAY = 5
    
    # ì¶œë ¥ ì„¤ì •
    OUTPUT_DIR = 'output'
    ENCODING = 'utf-8-sig'
```

### ì»¤ìŠ¤í…€ í¬ë¡¤ëŸ¬ ìƒì„±
```python
class CustomProfileCrawler(ProfileCrawler):
    def __init__(self, config=None):
        super().__init__()
        self.config = config or CrawlerConfig()
    
    def crawl_with_custom_fields(self, seller_name):
        """ì‚¬ìš©ì ì •ì˜ í•„ë“œ ì¶”ê°€ í¬ë¡¤ë§"""
        profile_data = self.crawl_seller_profile(seller_name)
        
        # ì¶”ê°€ í•„ë“œ ìˆ˜ì§‘
        profile_data.update({
            'response_time': self._extract_response_time(),
            'completion_rate': self._extract_completion_rate(),
            'rating_score': self._extract_rating_score()
        })
        
        return profile_data
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì˜ˆì œ
```python
import unittest
from crawler import SellersCrawler, ProfileCrawler

class TestCrawlerFunctions(unittest.TestCase):
    
    def setUp(self):
        self.sellers_crawler = SellersCrawler()
        self.profile_crawler = ProfileCrawler()
    
    def test_driver_setup(self):
        """WebDriver ì •ìƒ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        self.assertIsNotNone(self.sellers_crawler.driver)
        self.assertEqual(self.sellers_crawler.driver.name, 'chrome')
    
    def test_single_profile_crawl(self):
        """ë‹¨ì¼ í”„ë¡œí•„ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
        test_seller = "test_seller_name"
        profile = self.profile_crawler.crawl_seller_profile(test_seller)
        
        self.assertIn('seller_name', profile)
        self.assertEqual(profile['seller_name'], test_seller)
    
    def tearDown(self):
        self.sellers_crawler.close()
        self.profile_crawler.close()

# ì‹¤í–‰
if __name__ == '__main__':
    unittest.main()
```

### ë°ì´í„° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
```python
def validate_crawled_data(csv_file_path):
    """ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ í’ˆì§ˆ ê²€ì¦"""
    import pandas as pd
    
    df = pd.read_csv(csv_file_path)
    
    validation_results = {
        'total_records': len(df),
        'missing_seller_names': df['seller_name'].isnull().sum(),
        'missing_profiles': df['profile_url'].isnull().sum(),
        'empty_introductions': (df['introduction'] == '').sum(),
        'average_reviews': df['total_reviews'].mean() if 'total_reviews' in df else 0
    }
    
    print("ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ê²°ê³¼:")
    for key, value in validation_results.items():
        print(f"  {key}: {value}")
    
    return validation_results
```

## ğŸ”„ ì§€ì†ì ì¸ ìœ ì§€ë³´ìˆ˜

### ì •ê¸° ì ê²€ ì‚¬í•­
1. **XPath ì„ íƒì ìœ íš¨ì„±**: í¬ëª½ ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ ì‹œ ì—…ë°ì´íŠ¸ í•„ìš”
2. **ëŒ€ê¸° ì‹œê°„ ì¡°ì •**: ì‚¬ì´íŠ¸ ì‘ë‹µ ì†ë„ ë³€í™”ì— ë”°ë¥¸ ìµœì í™”
3. **ì˜¤ë¥˜ìœ¨ ëª¨ë‹ˆí„°ë§**: ì‹¤íŒ¨ìœ¨ì´ ë†’ì•„ì§€ë©´ ì„ íƒì ì¬ê²€í† 

### ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸
```python
def health_check_crawl():
    """í¬ë¡¤ëŸ¬ ìƒíƒœ í™•ì¸"""
    test_cases = [
        ('ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘', lambda: SellersCrawler().crawl_category_sellers(661)),
        ('í”„ë¡œí•„ ìˆ˜ì§‘', lambda: ProfileCrawler().crawl_seller_profile('test_user')),
        ('ë¦¬ë·° ìˆ˜ì§‘', lambda: ProfileCrawler().crawl_reviews('test_user', max_pages=1))
    ]
    
    results = {}
    for test_name, test_func in test_cases:
        try:
            start_time = time.time()
            result = test_func()
            end_time = time.time()
            
            results[test_name] = {
                'status': 'SUCCESS' if result else 'EMPTY',
                'duration': round(end_time - start_time, 2),
                'data_count': len(result) if isinstance(result, list) else 1
            }
        except Exception as e:
            results[test_name] = {
                'status': 'FAILED',
                'error': str(e)
            }
    
    return results
```

## ğŸš€ ì‹¤ì œ ìš´ì˜ ì˜ˆì œ

### í”„ë¡œë•ì…˜ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
```python
#!/usr/bin/env python3
"""
í¬ëª½ í¬ë¡¤ëŸ¬ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import argparse
import logging
from datetime import datetime
from crawler import SellersCrawler, ProfileCrawler

def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    log_filename = f"crawl_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

def main():
    parser = argparse.ArgumentParser(description='í¬ëª½ í¬ë¡¤ëŸ¬ ì‹¤í–‰')
    parser.add_argument('--category', type=int, default=661, help='ì¹´í…Œê³ ë¦¬ ID')
    parser.add_argument('--limit', type=int, help='ìˆ˜ì§‘í•  í”„ë¡œí•„ ìˆ˜ ì œí•œ')
    parser.add_argument('--reviews', action='store_true', help='ë¦¬ë·° í¬í•¨ ìˆ˜ì§‘')
    parser.add_argument('--services', action='store_true', help='ì„œë¹„ìŠ¤ ì •ë³´ í¬í•¨ ìˆ˜ì§‘')
    
    args = parser.parse_args()
    logger = setup_logging()
    
    try:
        logger.info(f"í¬ë¡¤ë§ ì‹œì‘ - ì¹´í…Œê³ ë¦¬: {args.category}")
        
        # Step 1: íŒë§¤ì ëª©ë¡ ìˆ˜ì§‘
        sellers_crawler = SellersCrawler()
        sellers = sellers_crawler.crawl_category_sellers(args.category)
        sellers_crawler.close()
        
        logger.info(f"íŒë§¤ì {len(sellers)}ëª… ìˆ˜ì§‘ ì™„ë£Œ")
        
        # Step 2: í”„ë¡œí•„ ìƒì„¸ ìˆ˜ì§‘
        profile_crawler = ProfileCrawler()
        seller_names = [s['seller_name'] for s in sellers]
        
        if args.limit:
            seller_names = seller_names[:args.limit]
        
        if args.reviews:
            profiles = profile_crawler.crawl_multiple_profiles_with_reviews(seller_names)
        else:
            profiles = profile_crawler.crawl_multiple_profiles(seller_names)
        
        logger.info(f"í”„ë¡œí•„ {len(profiles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # Step 3: ì„œë¹„ìŠ¤ ì •ë³´ ìˆ˜ì§‘ (ì„ íƒì )
        if args.services:
            logger.warning("ì„œë¹„ìŠ¤ ì •ë³´ ìˆ˜ì§‘ì€ í˜„ì¬ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            # ì„œë¹„ìŠ¤ ìˆ˜ì§‘ ë¡œì§...
        
        profile_crawler.close()
        logger.info("í¬ë¡¤ë§ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

if __name__ == "__main__":
    main()
```

### ì‹¤í–‰ ëª…ë ¹ì–´ ì˜ˆì œ
```bash
# ê¸°ë³¸ í”„ë¡œí•„ ìˆ˜ì§‘
python crawler_main.py --category 661 --limit 20

# ë¦¬ë·° í¬í•¨ ìˆ˜ì§‘
python crawler_main.py --category 661 --limit 10 --reviews

# ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ (ì„œë¹„ìŠ¤ í¬í•¨)
python crawler_main.py --category 661 --limit 5 --reviews --services
```

## ğŸ“ ì§€ì› ë° ê¸°ì—¬

í˜„ì¬ ì„œë¹„ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ê¸°ëŠ¥ì— ì´ìŠˆê°€ ìˆìœ¼ë¯€ë¡œ, í•´ë‹¹ ë¶€ë¶„ì˜ ê°œì„ ì— ê¸°ì—¬í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.

**ì£¼ìš” ê°œì„  ì˜ì—­**:
- `_extract_package_prices()` ë©”ì„œë“œ ì•ˆì •ì„± í–¥ìƒ
- `_get_service_list()` ë©”ì„œë“œ URL ìˆ˜ì§‘ ë¡œì§ ê°œì„   
- ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° ì‹œê°„ ìµœì í™”

**ê¸°ì—¬ ë°©ë²•**:
1. ì´ìŠˆ ë¦¬í¬íŠ¸: êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ìƒí™©ê³¼ ë¡œê·¸ ì œê³µ
2. ì½”ë“œ ê°œì„ : ì„œë¹„ìŠ¤ ìˆ˜ì§‘ ê´€ë ¨ ë©”ì„œë“œ ìˆ˜ì • ì‚¬í•­
3. í…ŒìŠ¤íŠ¸: ë‹¤ì–‘í•œ íŒë§¤ì í”„ë¡œí•„ì—ì„œì˜ ë™ì‘ ê²€ì¦